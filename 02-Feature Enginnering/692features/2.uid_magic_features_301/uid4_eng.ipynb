{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#  Libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "# Data processing, metrics and modeling\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold,KFold\n",
    "#from bayes_opt import BayesianOptimization\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "# Lgbm\n",
    "#import lightgbm as lgb\n",
    "# Suppr warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import itertools\n",
    "from scipy import interp\n",
    "\n",
    "# Plots\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.options.display.max_columns = None\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction = pd.read_csv('train_transaction.csv', index_col='TransactionID')\n",
    "test_transaction = pd.read_csv('test_transaction.csv', index_col='TransactionID')\n",
    "train_identity = pd.read_csv('train_identity.csv', index_col='TransactionID')\n",
    "test_identity = pd.read_csv('test_identity.csv', index_col='TransactionID')\n",
    "sample_submission = pd.read_csv('sample_submission.csv', index_col='TransactionID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MERGE, MISSING VALUE, FILL NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape : (590540, 433)\n",
      "Test shape  : (506691, 432)\n"
     ]
    }
   ],
   "source": [
    "# merge \n",
    "train_df = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\n",
    "test_df = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n",
    "\n",
    "print(\"Train shape : \"+str(train_df.shape))\n",
    "print(\"Test shape  : \"+str(test_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Establishing connection with: quadrical-database.mysql.database.azure.com\n",
      "Connection established\n",
      "Establishing connection with: quadrical-database.mysql.database.azure.com\n",
      "Connection established\n"
     ]
    }
   ],
   "source": [
    "print('starting')\n",
    "import sys\n",
    "#sys.path.insert(0 ,\"/home/notebooks/goMMT\")\n",
    "from goofy.base.base_utils import logutils as log\n",
    "from goofy.fileservice import FileServiceHelper\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import backend as K, optimizers, Model\n",
    "from keras.layers import Concatenate, Activation, Dense, Flatten, LSTM, RepeatVector, Dropout, TimeDistributed, GRU, Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import model_from_json, Sequential\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau,CSVLogger\n",
    "from keras.regularizers import l2\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from dask import dataframe as dd\n",
    "from dask.multiprocessing import get\n",
    "from multiprocessing import cpu_count\n",
    "nCores=cpu_count()\n",
    "\n",
    "#from QuadricalViews import QuadricalViews\n",
    "#from MakemytripUtils import *\n",
    "from QuadricalAdmin import *\n",
    "from PipelineAdmin import *\n",
    "from SessionAdmin import *\n",
    "from FeedFile import *\n",
    "from QuadricaldbStore import *\n",
    "#session1.log('this build has session logging working...')\n",
    "maxfile_count=1000\n",
    "minfile_rows=100\n",
    "min_price_delta=10\n",
    "isNew, useCaseId, pipelineId, file_name,sessionId = (0,1,10016, 'shopclues_combined_full.csv', # '../hugh3/shopclues_financial_clean_large.csv',\n",
    "                                                     'c05568da-dea5-4deb-a86e-9836b72a711f') ### Shopclues\n",
    "\n",
    "if isNew==1: sessionId=None\n",
    "deltaMode=0\n",
    "db1=QuadricaldbStore()\n",
    "config1=QuadricalAdmin()\n",
    "pipeline1=PipelineAdmin(useCaseId,pipelineId)\n",
    "session1=pipeline1.locateSession(sessionId)\n",
    "# session1.setcolumnRules(json.loads(config1.getcolumnRules(pipelineId)))\n",
    "# session1.feedRules = json.loads(config1.getfeedRules(pipelineId))\n",
    "# sessionId=session1.sessionId\n",
    "# session1.deltaMode=0\n",
    "# session1.cachedItems={}\n",
    "# lag_steps=4\n",
    "# fut_steps=6\n",
    "# deltaMode=1\n",
    "# feedfile1=FeedFile(session1)\n",
    "# scoringMode=0\n",
    "# #feedfile1.all_data_df=feedfile1.session1.get_object(\"gommt-20007-Train-combined-df\",\"pre-target-compute-df-for-varun\"+runId,deltaMode=1)\n",
    "# #feedfile1.process_feed_float_to_category(filterCols=['departure_time','arrival_time'])\n",
    "# #feedfile1.process_feed_categories_to_hot(deltaMode=0, pipelineId=20007,filterCols=['departure_time', 'arrival_time', 'departure_date_day_of_week','booking_date_day_of_week', 'flight_no_1'])\n",
    "# #feedfile1.session1.add_object_as_file(\"gommt-20007-Train-combined-df\",\"pre-target-compute-df-for-varun\"+runId,feedfile1.all_data_df)\n",
    "# #savedf=feedfile1.all_data_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file 58ea243e-6bbe-40a2-9b72-aaf902fc16f1\n",
      "reading file 6be49d98-9969-4ede-975c-9caea5f93490\n"
     ]
    }
   ],
   "source": [
    "train_df_added = session1.get_object('ae_reconstruction_error_811', 'train_df_added') # not work\n",
    "test_df_added = session1.get_object('ae_reconstruction_error_811', 'test_df_added')\n",
    "\n",
    "train_df_added.index = train_df.index\n",
    "test_df_added.index = test_df.index\n",
    "\n",
    "train_df = pd.concat([train_df, train_df_added], axis = 1)\n",
    "test_df = pd.concat([test_df, test_df_added], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New groupby\n",
    "train_df['days_passed'] = train_df['TransactionDT'] // 86400\n",
    "test_df['days_passed'] = test_df['TransactionDT'] // 86400\n",
    "train_df['days_created'] = train_df['days_passed'] - train_df['D1']\n",
    "test_df['days_created'] = test_df['days_passed'] - test_df['D1']\n",
    "\n",
    "train_df['uid4'] = train_df['card1'].astype(str) + '_' + train_df['ProductCD'].astype(str) +'_' + train_df['P_emaildomain'].astype(str) + '_' + \\\n",
    "                train_df['addr1'].astype(str) + '_' + train_df['days_created'].astype(str)\n",
    "test_df['uid4'] = test_df['card1'].astype(str) + '_' + test_df['ProductCD'].astype(str) +'_' + test_df['P_emaildomain'].astype(str) + '_' + \\\n",
    "                test_df['addr1'].astype(str) + '_' + test_df['days_created'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_GroupBy_mean(df, by,  window = 10, target = 'TransactionAmt'):\n",
    "    for fea_by in by:\n",
    "        print('Processing', fea_by)\n",
    "        #df[fea_by + target +'minus_mean_window'] =  df[target] - df.groupby(fea_by)[target].transform(lambda x: x.rolling(window, 1).mean())\n",
    "        df[fea_by + target +'minus_mean_all'] =   df[target] - df.groupby(fea_by)[target].transform('mean')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_list = ['TransactionAmt', 'D15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_list  =['TransactionAmt',\n",
    " 'dist1',\n",
    " 'dist2',\n",
    " 'C1',\n",
    " 'C2',\n",
    " 'C3',\n",
    " 'C4',\n",
    " 'C5',\n",
    " 'C6',\n",
    " 'C7',\n",
    " 'C8',\n",
    " 'C9',\n",
    " 'C10',\n",
    " 'C11',\n",
    " 'C12',\n",
    " 'C13',\n",
    " 'C14',\n",
    " 'D1',\n",
    " 'D2',\n",
    " 'D3',\n",
    " 'D4',\n",
    " 'D5',\n",
    " 'D6',\n",
    " 'D7',\n",
    " 'D8',\n",
    " 'D9',\n",
    " 'D10',\n",
    " 'D11',\n",
    " 'D12',\n",
    " 'D13',\n",
    " 'D14',\n",
    " 'D15',\n",
    " 'id_02',\n",
    " 'id_05',\n",
    " 'id_06',\n",
    " 'nulls1',\n",
    " 'V307',\n",
    " 'V310',\n",
    " 'V313',\n",
    " 'C5_fq_enc',\n",
    " 'P_emaildomain_bin',\n",
    " 'card5_count_full',\n",
    " 'card5_TransactionAmt_std',\n",
    " 'V315',\n",
    " 'id_02_to_mean_card1',\n",
    " 'C11_fq_enc',\n",
    " 'C9_fq_enc',\n",
    " 'D15_to_mean_addr1',\n",
    " 'D15_to_mean_card4',\n",
    " 'card5_TransactionAmt_mean',\n",
    " 'C2_fq_enc',\n",
    " 'D15_to_std_card1',\n",
    " 'C14_fq_enc',\n",
    " 'C6_fq_enc',\n",
    " 'id_31_count_dist',\n",
    " 'Transaction_day_of_week',\n",
    " 'C1_fq_enc',\n",
    " 'uid_TransactionAmt_std',\n",
    " 'D15_to_mean_card1',\n",
    " 'TransactionAmt_decimal',\n",
    " 'card2_TransactionAmt_std',\n",
    " 'TransactionAmt_to_mean_card4',\n",
    " 'uid_TransactionAmt_mean',\n",
    " 'C13_fq_enc',\n",
    " 'Transaction_hour_of_day',\n",
    " 'TransactionAmt_to_std_card1',\n",
    " 'TransactionAmt_to_std_card4',\n",
    " 'card1_TransactionAmt_std',\n",
    " 'card2_TransactionAmt_mean',\n",
    " 'V13TransactionAmtminus_mean_all',\n",
    " 'TransactionAmt_to_mean_card1',\n",
    " 'card1_TransactionAmt_mean',\n",
    " 'uid2_TransactionAmt_std',\n",
    " 'card2_fq_enc',\n",
    " 'uid2_TransactionAmt_mean',\n",
    " 'card1_count_full']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_added = pd.read_pickle('train_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_added = pd.read_pickle('test_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590540, 791)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_added.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features_added = []\n",
    "\n",
    "for fea in num_list:\n",
    "    if fea in train_df_added and fea not in train_df:\n",
    "        #print(fea)\n",
    "        features_added.append(fea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_added = train_df_added[features_added]\n",
    "test_df_added = test_df_added[features_added]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_added = [fea for fea in train_df_added if fea not in train_df]\n",
    "\n",
    "train_df_added = train_df_added[features_added]\n",
    "test_df_added = test_df_added[features_added]\n",
    "\n",
    "train_df_added.index = train_df.index\n",
    "test_df_added.index = test_df.index\n",
    "\n",
    "train_df = pd.concat([train_df, train_df_added], axis = 1)\n",
    "test_df = pd.concat([test_df, test_df_added], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590540, 455)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(506691, 454)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_list = [fea for fea in num_list if fea in train_df ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TransactionAmt',\n",
       " 'dist1',\n",
       " 'dist2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C3',\n",
       " 'C4',\n",
       " 'C5',\n",
       " 'C6',\n",
       " 'C7',\n",
       " 'C8',\n",
       " 'C9',\n",
       " 'C10',\n",
       " 'C11',\n",
       " 'C12',\n",
       " 'C13',\n",
       " 'C14',\n",
       " 'D1',\n",
       " 'D2',\n",
       " 'D3',\n",
       " 'D4',\n",
       " 'D5',\n",
       " 'D6',\n",
       " 'D7',\n",
       " 'D8',\n",
       " 'D9',\n",
       " 'D10',\n",
       " 'D11',\n",
       " 'D12',\n",
       " 'D13',\n",
       " 'D14',\n",
       " 'D15',\n",
       " 'id_02',\n",
       " 'id_05',\n",
       " 'id_06',\n",
       " 'V307',\n",
       " 'V310',\n",
       " 'V313',\n",
       " 'C5_fq_enc',\n",
       " 'card5_TransactionAmt_std',\n",
       " 'V315',\n",
       " 'C11_fq_enc',\n",
       " 'C9_fq_enc',\n",
       " 'card5_TransactionAmt_mean',\n",
       " 'C2_fq_enc',\n",
       " 'C14_fq_enc',\n",
       " 'C6_fq_enc',\n",
       " 'C1_fq_enc',\n",
       " 'uid_TransactionAmt_std',\n",
       " 'card2_TransactionAmt_std',\n",
       " 'uid_TransactionAmt_mean',\n",
       " 'C13_fq_enc',\n",
       " 'card1_TransactionAmt_std',\n",
       " 'card2_TransactionAmt_mean',\n",
       " 'card1_TransactionAmt_mean',\n",
       " 'uid2_TransactionAmt_std',\n",
       " 'card2_fq_enc',\n",
       " 'uid2_TransactionAmt_mean']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df['isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590540, 455)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(506691, 455)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train_df, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1097231, 455)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2',\n",
       "       'card3', 'card4', 'card5', 'card6', 'addr1',\n",
       "       ...\n",
       "       'card2_TransactionAmt_std', 'uid_TransactionAmt_mean', 'C13_fq_enc',\n",
       "       'card1_TransactionAmt_std', 'card2_TransactionAmt_mean',\n",
       "       'card1_TransactionAmt_mean', 'uid2_TransactionAmt_std', 'card2_fq_enc',\n",
       "       'uid2_TransactionAmt_mean', 'ae_reconstruction_error'],\n",
       "      dtype='object', length=455)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fea_by_list = ['uid4']\n",
    "# target_list = num_list\n",
    "target_list = ['ae_reconstruction_error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fea_by_list)\n",
    "len(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Processing ae_reconstruction_error uid4\n"
     ]
    }
   ],
   "source": [
    "columns_name = []\n",
    "added = []\n",
    "\n",
    "for target in target_list:\n",
    "    for fea_by in fea_by_list:\n",
    "        print('Now Processing', target, fea_by)\n",
    "        added.append(  data[target] - data.groupby(fea_by)[target].transform('mean')  )\n",
    "        added.append(  data[target] / data.groupby(fea_by)[target].transform('mean')  )\n",
    "        added.append(  data[target] / data.groupby(fea_by)[target].transform('std')  )\n",
    "        columns_name.append(fea_by + '_' + target +'_' + '_minus_mean')\n",
    "        columns_name.append(fea_by + '_' + target +'_' + '_divide_mean')\n",
    "        columns_name.append(fea_by + '_' + target +'_' + '_divide_std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1097231, 3)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid4_ae_reconstruction_error__minus_mean</th>\n",
       "      <th>uid4_ae_reconstruction_error__divide_mean</th>\n",
       "      <th>uid4_ae_reconstruction_error__divide_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000231</td>\n",
       "      <td>0.930559</td>\n",
       "      <td>12.270024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001134</td>\n",
       "      <td>1.108609</td>\n",
       "      <td>2.952558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid4_ae_reconstruction_error__minus_mean  \\\n",
       "0                                  0.000000   \n",
       "1                                  0.000000   \n",
       "2                                 -0.000231   \n",
       "3                                  0.001134   \n",
       "4                                  0.000000   \n",
       "\n",
       "   uid4_ae_reconstruction_error__divide_mean  \\\n",
       "0                                   1.000000   \n",
       "1                                   1.000000   \n",
       "2                                   0.930559   \n",
       "3                                   1.108609   \n",
       "4                                   1.000000   \n",
       "\n",
       "   uid4_ae_reconstruction_error__divide_std  \n",
       "0                                       NaN  \n",
       "1                                       NaN  \n",
       "2                                 12.270024  \n",
       "3                                  2.952558  \n",
       "4                                       NaN  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_added = pd.DataFrame( np.stack(added, axis = 1) )\n",
    "data_added.columns = columns_name\n",
    "\n",
    "data_added.shape\n",
    "data_added.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_added_back = deepcopy(data_added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1097231, 174)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_added_back.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_added.to_pickle('added_Groupby_divide&minus_uid4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fea_by_list)\n",
    "len(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Processing ae_reconstruction_error uid4\n"
     ]
    }
   ],
   "source": [
    "columns_name = []\n",
    "added = []\n",
    "\n",
    "for target in target_list:\n",
    "    for fea_by in fea_by_list:\n",
    "        print('Now Processing', target, fea_by)\n",
    "        added.append(data.groupby(fea_by)[target].transform('mean'))\n",
    "        added.append(data.groupby(fea_by)[target].transform('std'))\n",
    "        columns_name.append(fea_by + '_' + target +'_' + 'mean')\n",
    "        columns_name.append(fea_by + '_' + target +'_' + 'std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1097231, 2)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid4_ae_reconstruction_error_mean</th>\n",
       "      <th>uid4_ae_reconstruction_error_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.006511</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001674</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003331</td>\n",
       "      <td>0.000253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.010437</td>\n",
       "      <td>0.003919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009007</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid4_ae_reconstruction_error_mean  uid4_ae_reconstruction_error_std\n",
       "0                           0.006511                               NaN\n",
       "1                           0.001674                               NaN\n",
       "2                           0.003331                          0.000253\n",
       "3                           0.010437                          0.003919\n",
       "4                           0.009007                               NaN"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_added = pd.DataFrame( np.stack(added, axis = 1) )\n",
    "data_added.columns = columns_name\n",
    "\n",
    "data_added.shape\n",
    "data_added.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_added.to_pickle('Groupby_uid4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_added_back = pd.concat([data_added, data_added_back], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1097231, 295)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_added_back.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_added_back.to_pickle('all_uid4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_lgb = session1.get_object('25pred', 'oof')\n",
    "predictions_lgb = session1.get_object('25pred', 'predictions')\n",
    "\n",
    "train_df['pred'] = oof_lgb\n",
    "test_df['pred'] = predictions_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# invest = ['ProductCD', 'card1', 'card2','card3','card4','card5','card6','dist1', 'dist2',\n",
    "#           'addr1','addr2',\n",
    "#           'P_emaildomain',\n",
    "#           'R_emaildomain']\n",
    "\n",
    "# for fea in invest:\n",
    "#     print(fea)\n",
    "#     train_df[fea] = train_df[fea].astype(str)\n",
    "#     test_df[fea] = test_df[fea].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invest = ['ProductCD', 'card1', 'card2','card3','card4','card5','card6','dist1', 'dist2',\n",
    "          'addr1','addr2',\n",
    "          'P_emaildomain',\n",
    "          'R_emaildomain']\n",
    "\n",
    "for fea in invest:\n",
    "    print(fea, train_df[fea].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df['days_passed'] = train_df['TransactionDT'] // 86400\n",
    "train_df['days_created'] = train_df['days_passed']  - train_df['D1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['uid4'] = train_df['card1'].astype(str)+'_'+train_df['ProductCD'].astype(str)+ '_'+train_df['P_emaildomain'].astype(str)+'_'+train_df['addr1'].astype(str)+'_' + train_df['days_created'].astype(str) # 9479"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['uid4'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df['uid4'][train_df.isFraud == 1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train_df['isFraud'][ train_df['uid4'] == '12577_W_gmail.com_299.0_150.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['uid4'].isnull().sum() # only this 0.9479\n",
    "train_df.loc[train_df['addr1'].isnull(), 'uid4'] = np.nan\n",
    "train_df['uid4'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['uid4'].isnull().sum() # do not add this 9471\n",
    "# train_df.loc[train_df['P_emaildomain'].isnull(), 'uid4'] = np.nan\n",
    "# train_df['uid4'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform(lambda x: x.rolling(window, 1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['ProductCD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['uid4'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "window = 5\n",
    "temp_dict_prev = train_df.groupby('uid4')['pred'].rolling(window, 1).mean().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 5\n",
    "temp_dict_back = train_df[::-1].groupby('uid4')['pred'].rolling(window, 1).mean().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['index'] = train_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['sep'] = '&'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['uid4+index'] = train_df['uid4'] + train_df['sep']  +  train_df['index'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tmp(x):\n",
    "    return (x.split('&')[0], int(x.split('&')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['uid4+index'][~train_df['uid4+index'].isnull()] = train_df['uid4+index'][~train_df['uid4+index'].isnull()].apply(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dict_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9482\n",
    "temp_dict = train_df.groupby('uid4')['pred'].mean().to_dict()\n",
    "train_df.loc[~(train_df['uid4'].isnull()), 'pred2'] = 0.15 * train_df['uid4+index'].map(temp_dict_back) + 0.15 * train_df['uid4+index'].map(temp_dict_prev) + 0.7 * train_df['uid4'].map(temp_dict)\n",
    "#train_df.loc[~(train_df['uid4'].isnull()), 'pred2'] = train_df['uid4+index'].map(temp_dict_prev)\n",
    "train_df.loc[train_df['uid4'].isnull(), 'pred2'] = train_df['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9482\n",
    "#temp_dict = train_df.groupby('uid4')['pred'].mean().to_dict()\n",
    "train_df.loc[~(train_df['uid4'].isnull()), 'pred2'] = 0.5 * train_df['uid4+index'].map(temp_dict_back) + 0.5 * train_df['uid4+index'].map(temp_dict_prev)\n",
    "#train_df.loc[~(train_df['uid4'].isnull()), 'pred2'] = train_df['uid4+index'].map(temp_dict_prev)\n",
    "train_df.loc[train_df['uid4'].isnull(), 'pred2'] = train_df['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['uid4'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['pred2'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dict = train_df.groupby('uid4')['pred'].mean().to_dict()\n",
    "train_df.loc[~(train_df['uid4'].isnull()), 'pred2'] = train_df['uid4'].map(temp_dict)\n",
    "train_df.loc[train_df['uid4'].isnull(), 'pred2'] = train_df['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    features = ['pred2']\n",
    "\n",
    "    start = time()\n",
    "    plt.rcParams[\"axes.grid\"] = True\n",
    "    nfold = 5\n",
    "    #skf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=42)\n",
    "    skf = KFold(n_splits=nfold, shuffle=False, random_state=42)\n",
    "\n",
    "    oof = np.zeros(len(train_df))\n",
    "    mean_fpr = np.linspace(0,1,100)\n",
    "    cms= []\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    y_real = []\n",
    "    y_proba = []\n",
    "    recalls = []\n",
    "    roc_aucs = []\n",
    "    f1_scores = []\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    predictions = np.zeros(len(test_df))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "\n",
    "    i = 1\n",
    "#     clf = lgb.LGBMClassifier()\n",
    "#     clf.set_params(**param_lgb)\n",
    "    #clf = LogisticRegression()\n",
    "    \n",
    "    \n",
    "    for train_idx, valid_idx in skf.split(train_df, train_df.isFraud.values):\n",
    "        print(\"\\nfold {}\".format(i))\n",
    "        x_train = train_df.iloc[train_idx][features]\n",
    "        y_train = train_df.iloc[train_idx][target]\n",
    "\n",
    "        x_val = train_df.iloc[valid_idx][features]\n",
    "        y_val = train_df.iloc[valid_idx][target]\n",
    "\n",
    "\n",
    "#         clf = lgb.LGBMClassifier()\n",
    "#         clf.set_params(**param_lgb)\n",
    "#         clf.fit(x_train, y_train,\n",
    "#                 eval_metric =['auc'],\n",
    "#                 verbose = 10,\n",
    "#                 eval_set = (x_val, y_val)) \n",
    "\n",
    "\n",
    "        oof[valid_idx] = x_val['pred2']\n",
    "\n",
    "        #predictions += clf.predict_proba(test_df[features])[:,1]  / nfold\n",
    "\n",
    "\n",
    "        # Scores \n",
    "        print(roc_auc_score(train_df.iloc[valid_idx][target].values, oof[valid_idx]))\n",
    "        \n",
    "        roc_aucs.append(roc_auc_score(train_df.iloc[valid_idx][target].values, oof[valid_idx]))\n",
    "        accuracies.append(accuracy_score(train_df.iloc[valid_idx][target].values, oof[valid_idx].round()))\n",
    "        recalls.append(recall_score(train_df.iloc[valid_idx][target].values, oof[valid_idx].round()))\n",
    "        precisions.append(precision_score(train_df.iloc[valid_idx][target].values ,oof[valid_idx].round()))\n",
    "        f1_scores.append(f1_score(train_df.iloc[valid_idx][target].values, oof[valid_idx].round()))\n",
    "\n",
    "        # Roc curve by folds\n",
    "        f = plt.figure(1)\n",
    "        fpr, tpr, t = roc_curve(train_df.iloc[valid_idx][target].values, oof[valid_idx])\n",
    "        tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "        plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.4f)' % (i,roc_auc))\n",
    "\n",
    "        # Precion recall by folds\n",
    "        g = plt.figure(2)\n",
    "        precision, recall, _ = precision_recall_curve(train_df.iloc[valid_idx][target].values, oof[valid_idx])\n",
    "        y_real.append(train_df.iloc[valid_idx][target].values)\n",
    "        y_proba.append(oof[valid_idx])\n",
    "        plt.plot(recall, precision, lw=2, alpha=0.3, label='P|R fold %d' % (i))  \n",
    "\n",
    "        i= i+1\n",
    "\n",
    "        # Confusion matrix by folds\n",
    "        cms.append(confusion_matrix(train_df.iloc[valid_idx][target].values, oof[valid_idx].round()))\n",
    "\n",
    "#         # Features imp\n",
    "#         fold_importance_df = pd.DataFrame()\n",
    "#         fold_importance_df[\"Feature\"] = features\n",
    "#         fold_importance_df[\"importance\"] = pd.DataFrame(sorted(zip(clf.feature_importances_,x_train.columns)), \n",
    "#                                                         columns=['Value','Feature'])['Value']\n",
    "#         fold_importance_df[\"fold\"] = nfold + 1\n",
    "#         feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        \n",
    "\n",
    "        # Metrics\n",
    "    print(\n",
    "        '\\nCV roc score        : {0:.4f}, std: {1:.4f}.'.format(np.mean(roc_aucs), np.std(roc_aucs)),\n",
    "        '\\nCV accuracy score   : {0:.4f}, std: {1:.4f}.'.format(np.mean(accuracies), np.std(accuracies)),\n",
    "        '\\nCV recall score     : {0:.4f}, std: {1:.4f}.'.format(np.mean(recalls), np.std(recalls)),\n",
    "        '\\nCV precision score  : {0:.4f}, std: {1:.4f}.'.format(np.mean(precisions), np.std(precisions)),\n",
    "        '\\nCV f1 score         : {0:.4f}, std: {1:.4f}.'.format(np.mean(f1_scores), np.std(f1_scores))\n",
    "        )\n",
    "    length = time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = '2017-12-01'\n",
    "startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n",
    "\n",
    "train_df['TransactionDT'] = train_df['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n",
    "\n",
    "train_df['Date'] = train_df['TransactionDT'].apply(lambda x: str(x)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['TransactionDT'] = test_df['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n",
    "\n",
    "test_df['Date'] = test_df['TransactionDT'].apply(lambda x: str(x)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['uid4'] = train_df['card1'].astype(str)+'_'+train_df['ProductCD'].astype(str)+ '_'+train_df['P_emaildomain'].astype(str)+'_'+train_df['addr1'].astype(str)+'_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['uid4'] = test_df['card1'].astype(str)+'_'+test_df['ProductCD'].astype(str)+ '_'+test_df['P_emaildomain'].astype(str)+'_'+test_df['addr1'].astype(str)+'_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df['uid4'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.loc[train_df['addr1'].isnull(), 'uid4'] = np.nan\n",
    "test_df.loc[test_df['addr1'].isnull(), 'uid4'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[train_df['P_emaildomain'].isnull(), 'uid4'] = np.nan\n",
    "test_df.loc[test_df['P_emaildomain'].isnull(), 'uid4'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['uid4'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dict = train_df.groupby('uid4')['pred'].mean().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['pred2'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[~(train_df['uid4'].isnull()), 'pred2'] = train_df['uid4'].map(temp_dict)\n",
    "train_df.loc[train_df['uid4'].isnull(), 'pred2'] = train_df['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'isFraud'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_lgb = session1.get_object('25pred', 'oof')\n",
    "predictions_lgb = session1.get_object('25pred', 'predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['pred'] = oof_lgb\n",
    "test_df['pred'] = predictions_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    start = time()\n",
    "    plt.rcParams[\"axes.grid\"] = True\n",
    "    nfold = 5\n",
    "    #skf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=42)\n",
    "    skf = KFold(n_splits=nfold, shuffle=False, random_state=42)\n",
    "\n",
    "    oof = np.zeros(len(train_df))\n",
    "    mean_fpr = np.linspace(0,1,100)\n",
    "    cms= []\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    y_real = []\n",
    "    y_proba = []\n",
    "    recalls = []\n",
    "    roc_aucs = []\n",
    "    f1_scores = []\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    predictions = np.zeros(len(test_df))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "\n",
    "    i = 1\n",
    "#     clf = lgb.LGBMClassifier()\n",
    "#     clf.set_params(**param_lgb)\n",
    "    #clf = LogisticRegression()\n",
    "    \n",
    "    \n",
    "    for train_idx, valid_idx in skf.split(train_df, train_df.isFraud.values):\n",
    "        print(\"\\nfold {}\".format(i))\n",
    "        x_train = train_df.iloc[train_idx][features]\n",
    "        y_train = train_df.iloc[train_idx][target]\n",
    "\n",
    "        x_val = train_df.iloc[valid_idx][features]\n",
    "        y_val = train_df.iloc[valid_idx][target]\n",
    "\n",
    "\n",
    "#         clf = lgb.LGBMClassifier()\n",
    "#         clf.set_params(**param_lgb)\n",
    "#         clf.fit(x_train, y_train,\n",
    "#                 eval_metric =['auc'],\n",
    "#                 verbose = 10,\n",
    "#                 eval_set = (x_val, y_val)) \n",
    "\n",
    "\n",
    "        oof[valid_idx] = x_val['pred']\n",
    "\n",
    "        #predictions += clf.predict_proba(test_df[features])[:,1]  / nfold\n",
    "\n",
    "\n",
    "        # Scores \n",
    "        print(roc_auc_score(train_df.iloc[valid_idx][target].values, oof[valid_idx]))\n",
    "        \n",
    "        roc_aucs.append(roc_auc_score(train_df.iloc[valid_idx][target].values, oof[valid_idx]))\n",
    "        accuracies.append(accuracy_score(train_df.iloc[valid_idx][target].values, oof[valid_idx].round()))\n",
    "        recalls.append(recall_score(train_df.iloc[valid_idx][target].values, oof[valid_idx].round()))\n",
    "        precisions.append(precision_score(train_df.iloc[valid_idx][target].values ,oof[valid_idx].round()))\n",
    "        f1_scores.append(f1_score(train_df.iloc[valid_idx][target].values, oof[valid_idx].round()))\n",
    "\n",
    "        # Roc curve by folds\n",
    "        f = plt.figure(1)\n",
    "        fpr, tpr, t = roc_curve(train_df.iloc[valid_idx][target].values, oof[valid_idx])\n",
    "        tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "        plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.4f)' % (i,roc_auc))\n",
    "\n",
    "        # Precion recall by folds\n",
    "        g = plt.figure(2)\n",
    "        precision, recall, _ = precision_recall_curve(train_df.iloc[valid_idx][target].values, oof[valid_idx])\n",
    "        y_real.append(train_df.iloc[valid_idx][target].values)\n",
    "        y_proba.append(oof[valid_idx])\n",
    "        plt.plot(recall, precision, lw=2, alpha=0.3, label='P|R fold %d' % (i))  \n",
    "\n",
    "        i= i+1\n",
    "\n",
    "        # Confusion matrix by folds\n",
    "        cms.append(confusion_matrix(train_df.iloc[valid_idx][target].values, oof[valid_idx].round()))\n",
    "\n",
    "#         # Features imp\n",
    "#         fold_importance_df = pd.DataFrame()\n",
    "#         fold_importance_df[\"Feature\"] = features\n",
    "#         fold_importance_df[\"importance\"] = pd.DataFrame(sorted(zip(clf.feature_importances_,x_train.columns)), \n",
    "#                                                         columns=['Value','Feature'])['Value']\n",
    "#         fold_importance_df[\"fold\"] = nfold + 1\n",
    "#         feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        \n",
    "\n",
    "        # Metrics\n",
    "    print(\n",
    "        '\\nCV roc score        : {0:.4f}, std: {1:.4f}.'.format(np.mean(roc_aucs), np.std(roc_aucs)),\n",
    "        '\\nCV accuracy score   : {0:.4f}, std: {1:.4f}.'.format(np.mean(accuracies), np.std(accuracies)),\n",
    "        '\\nCV recall score     : {0:.4f}, std: {1:.4f}.'.format(np.mean(recalls), np.std(recalls)),\n",
    "        '\\nCV precision score  : {0:.4f}, std: {1:.4f}.'.format(np.mean(precisions), np.std(precisions)),\n",
    "        '\\nCV f1 score         : {0:.4f}, std: {1:.4f}.'.format(np.mean(f1_scores), np.std(f1_scores))\n",
    "        )\n",
    "    length = time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df['TransactionAmt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "invest = ['ProductCD', 'card1', 'card2','card3','card4','card5','card6','dist1', 'dist2',\n",
    "          'addr1','addr2',\n",
    "          'P_emaildomain',\n",
    "          'R_emaildomain']\n",
    "\n",
    "for fea in invest:\n",
    "    print(fea)\n",
    "    train_df[fea] = train_df[fea].astype(str)\n",
    "    test_df[fea] = test_df[fea].astype(str)\n",
    "    \n",
    "\n",
    "train_df['uid4'] = train_df['uid3'].astype(str)+'_'+train_df['P_emaildomain'].astype(str)\n",
    "test_df['uid4'] = test_df['uid3'].astype(str)+'_'+test_df['P_emaildomain'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['uid'] = train_df['card1'].astype(str)+'_'+train_df['card2'].astype(str)\n",
    "test_df['uid'] = test_df['card1'].astype(str)+'_'+test_df['card2'].astype(str)\n",
    "\n",
    "train_df['uid2'] = train_df['uid'].astype(str)+'_'+train_df['card3'].astype(str)+'_'+train_df['card5'].astype(str)\n",
    "test_df['uid2'] = test_df['uid'].astype(str)+'_'+test_df['card3'].astype(str)+'_'+test_df['card5'].astype(str)\n",
    "\n",
    "train_df['uid3'] = train_df['uid2'].astype(str)+'_'+train_df['addr1'].astype(str)+'_'+train_df['addr2'].astype(str)\n",
    "test_df['uid3'] = test_df['uid2'].astype(str)+'_'+test_df['addr1'].astype(str)+'_'+test_df['addr2'].astype(str)\n",
    "\n",
    "train_df['uid4'] = train_df['uid3'].astype(str)+'_'+train_df['P_emaildomain'].astype(str)\n",
    "test_df['uid4'] = test_df['uid3'].astype(str)+'_'+test_df['P_emaildomain'].astype(str)\n",
    "\n",
    "train_df['uid5'] = train_df['uid3'].astype(str)+'_'+train_df['R_emaildomain'].astype(str)\n",
    "test_df['uid5'] = test_df['uid3'].astype(str)+'_'+test_df['R_emaildomain'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['my_uid1'] = train_df['card1'] + train_df['card2']+ train_df['card3']+ train_df['card4']+ train_df['card5']+ train_df['card6']\n",
    "train_df['my_uid2'] = train_df['my_uid1'] + train_df['addr1']+ train_df['addr2'] \n",
    "train_df['my_uid3'] = train_df['my_uid2'] + train_df['P_emaildomain'] + train_df['R_emaildomain']\n",
    "train_df['my_uid4'] = train_df['my_uid3'] + train_df['dist1'] + train_df['dist2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['my_uid1'] = test_df['card1'] + test_df['card2']+ test_df['card3']+ test_df['card4']+ test_df['card5']+ test_df['card6']\n",
    "test_df['my_uid2'] = test_df['my_uid1'] + test_df['addr1']+ test_df['addr2'] \n",
    "test_df['my_uid3'] = test_df['my_uid2'] + test_df['P_emaildomain'] + test_df['R_emaildomain']\n",
    "test_df['my_uid4'] = test_df['my_uid3'] + test_df['dist1'] + test_df['dist2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_list = ['ProductCD', 'card1', 'card2','card3','card4','card5','card6','dist1', 'dist2',\n",
    "          'addr1','addr2',\n",
    "          'P_emaildomain',\n",
    "          'R_emaildomain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_list += list(train_df.columns[-9:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pre_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['TransactionAmt','TransactionDT','D15']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['D15'].fillna(999999, inplace =True)\n",
    "test_df['D15'].fillna(9999, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for target in ['TransactionAmt','TransactionDT','D15']:\n",
    "    for fea in pre_list:\n",
    "        print(fea, target)\n",
    "        train_df[fea + target + 'rank'] = train_df.groupby([fea])[target].rank(method='dense').astype(int)\n",
    "        test_df[fea + target + 'rank'] = test_df.groupby([fea])[target].rank(method='dense').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "use_list = list(train_df.columns[-len(pre_list)*3:])\n",
    "\n",
    "use_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(use_list)\n",
    "\n",
    "train_df_added = train_df[use_list]\n",
    "test_df_added = test_df[use_list]\n",
    "\n",
    "train_df_added.head()\n",
    "test_df_added.head()\n",
    "\n",
    "train_df_added.shape\n",
    "test_df_added.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_added = train_df_added.reset_index().iloc[:,1:]\n",
    "test_df_added = test_df_added.reset_index().iloc[:,1:]\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "train_df_added.head()\n",
    "test_df_added.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session1.add_object_as_file('rank', 'train_df_added', train_df_added)\n",
    "session1.add_object_as_file('rank', 'test_df_added', test_df_added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
